
@misc{shaham_singan_2019,
	title = {{SinGAN}: {Learning} a {Generative} {Model} from a {Single} {Natural} {Image}},
	shorttitle = {{SinGAN}},
	url = {http://arxiv.org/abs/1905.01164},
	doi = {10.48550/arXiv.1905.01164},
	abstract = {We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Shaham, Tamar Rott and Dekel, Tali and Michaeli, Tomer},
	month = sep,
	year = {2019},
	note = {arXiv:1905.01164 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, GANs},
	annote = {Comment: ICCV 2019},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/5MQVXJU3/Shaham et al. - 2019 - SinGAN Learning a Generative Model from a Single .pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/PZ6LM5Y6/1905.html:text/html;Texte intégral:/home/mraillat/snap/zotero-snap/common/Zotero/storage/KI3REZHU/Shaham et al. - 2019 - SinGAN Learning a Generative Model from a Single .pdf:application/pdf},
}

@misc{karras_training_2020,
	title = {Training {Generative} {Adversarial} {Networks} with {Limited} {Data}},
	url = {http://arxiv.org/abs/2006.06676},
	doi = {10.48550/arXiv.2006.06676},
	abstract = {Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
	month = oct,
	year = {2020},
	note = {arXiv:2006.06676 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, StyleGAN},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/L48BM23W/Karras et al. - 2020 - Training Generative Adversarial Networks with Limi.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/DK9RKZNH/2006.html:text/html},
}

@misc{karras_analyzing_2020,
	title = {Analyzing and {Improving} the {Image} {Quality} of {StyleGAN}},
	url = {http://arxiv.org/abs/1912.04958},
	doi = {10.48550/arXiv.1912.04958},
	abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	month = mar,
	year = {2020},
	note = {arXiv:1912.04958 [cs, eess, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, GANs, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, StyleGAN, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/UN9NVPWP/Karras et al. - 2020 - Analyzing and Improving the Image Quality of Style.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/Q82UCVV7/1912.html:text/html},
}

@inproceedings{karras_alias-free_2021,
	title = {Alias-{Free} {Generative} {Adversarial} {Networks}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/076ccd93ad68be51f23707988e934906-Abstract.html},
	abstract = {We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.},
	urldate = {2022-12-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Karras, Tero and Aittala, Miika and Laine, Samuli and Härkönen, Erik and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	year = {2021},
	keywords = {GANs, StyleGAN},
	pages = {852--863},
	file = {Full Text PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/FZZT9PKW/Karras et al. - 2021 - Alias-Free Generative Adversarial Networks.pdf:application/pdf},
}

@misc{karras_style-based_2019,
	title = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1812.04948},
	doi = {10.48550/arXiv.1812.04948},
	abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	month = mar,
	year = {2019},
	note = {arXiv:1812.04948 [cs, stat]},
	keywords = {GANs, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, StyleGAN},
	annote = {Comment: CVPR 2019 final version},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/XD94UT52/Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/TS8QEYJG/1812.html:text/html},
}

@misc{karras_progressive_2018,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {http://arxiv.org/abs/1710.10196},
	doi = {10.48550/arXiv.1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	month = feb,
	year = {2018},
	note = {arXiv:1710.10196 [cs, stat]},
	keywords = {GANs, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Final ICLR 2018 version},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/2SFNHAB6/Karras et al. - 2018 - Progressive Growing of GANs for Improved Quality, .pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/X45ECFBS/1710.html:text/html},
}

@misc{brock_large_2019,
	title = {Large {Scale} {GAN} {Training} for {High} {Fidelity} {Natural} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/1809.11096},
	doi = {10.48550/arXiv.1809.11096},
	abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
	month = feb,
	year = {2019},
	note = {arXiv:1809.11096 [cs, stat]},
	keywords = {GANs, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/2BNUECJP/Brock et al. - 2019 - Large Scale GAN Training for High Fidelity Natural.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/26J7UW8S/1809.html:text/html},
}

@misc{gulrajani_improved_2017,
	title = {Improved {Training} of {Wasserstein} {GANs}},
	url = {http://arxiv.org/abs/1704.00028},
	doi = {10.48550/arXiv.1704.00028},
	abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	month = dec,
	year = {2017},
	note = {arXiv:1704.00028 [cs, stat]},
	keywords = {GANs, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS camera-ready},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/36C5IIAF/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/NLPBMS6X/1704.html:text/html},
}

@misc{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	doi = {10.48550/arXiv.1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv:1511.06434 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, GANs, Computer Science - Machine Learning},
	annote = {Comment: Under review as a conference paper at ICLR 2016},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/TUR85MJ5/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/ID66E7AF/1511.html:text/html},
}

@misc{xia_gan_2022,
	title = {{GAN} {Inversion}: {A} {Survey}},
	shorttitle = {{GAN} {Inversion}},
	url = {http://arxiv.org/abs/2101.05278},
	doi = {10.48550/arXiv.2101.05278},
	abstract = {GAN inversion aims to invert a given image back into the latent space of a pretrained GAN model, for the image to be faithfully reconstructed from the inverted code by the generator. As an emerging technique to bridge the real and fake image domains, GAN inversion plays an essential role in enabling the pretrained GAN models such as StyleGAN and BigGAN to be used for real image editing applications. Meanwhile, GAN inversion also provides insights on the interpretation of GAN's latent space and how the realistic images can be generated. In this paper, we provide an overview of GAN inversion with a focus on its recent algorithms and applications. We cover important techniques of GAN inversion and their applications to image restoration and image manipulation. We further elaborate on some trends and challenges for future directions.},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Xia, Weihao and Zhang, Yulun and Yang, Yujiu and Xue, Jing-Hao and Zhou, Bolei and Yang, Ming-Hsuan},
	month = mar,
	year = {2022},
	note = {arXiv:2101.05278 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Inversion},
	annote = {Comment: papers on generative modeling: https://github.com/zhoubolei/awesome-generative-modeling awesome gan-inversion papers: https://github.com/weihaox/awesome-gan-inversion},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/9W8CCXDE/Xia et al. - 2022 - GAN Inversion A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/SZZCFBNR/2101.html:text/html},
}

@inproceedings{voynov_unsupervised_2020,
	title = {Unsupervised {Discovery} of {Interpretable} {Directions} in the {GAN} {Latent} {Space}},
	url = {https://proceedings.mlr.press/v119/voynov20a.html},
	abstract = {The latent spaces of GAN models often have semantically meaningful directions. Moving in these directions corresponds to human-interpretable image transformations, such as zooming or recoloring, enabling a more controllable generation process. However, the discovery of such directions is currently performed in a supervised manner, requiring human labels, pretrained models, or some form of self-supervision. These requirements severely restrict a range of directions existing approaches can discover. In this paper, we introduce an unsupervised method to identify interpretable directions in the latent space of a pretrained GAN model. By a simple model-agnostic procedure, we find directions corresponding to sensible semantic manipulations without any form of (self-)supervision. Furthermore, we reveal several non-trivial findings, which would be difficult to obtain by existing methods, e.g., a direction corresponding to background removal. As an immediate practical benefit of our work, we show how to exploit this finding to achieve competitive performance for weakly-supervised saliency detection. The implementation of our method is available online.},
	language = {en},
	urldate = {2022-12-16},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Voynov, Andrey and Babenko, Artem},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {Inversion},
	pages = {9786--9796},
	file = {Full Text PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/44PLPRN5/Voynov et Babenko - 2020 - Unsupervised Discovery of Interpretable Directions.pdf:application/pdf;Supplementary PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/5B3YCCIP/Voynov et Babenko - 2020 - Unsupervised Discovery of Interpretable Directions.pdf:application/pdf},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2022-12-16},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661 [cs, stat]},
	keywords = {GANs, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/JSVKJUFP/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/XS5BNJWE/1406.html:text/html},
}

@misc{harkonen_ganspace_2020,
	title = {{GANSpace}: {Discovering} {Interpretable} {GAN} {Controls}},
	shorttitle = {{GANSpace}},
	url = {http://arxiv.org/abs/2004.02546},
	doi = {10.48550/arXiv.2004.02546},
	abstract = {This paper describes a simple technique to analyze Generative Adversarial Networks (GANs) and create interpretable controls for image synthesis, such as change of viewpoint, aging, lighting, and time of day. We identify important latent directions based on Principal Components Analysis (PCA) applied either in latent space or feature space. Then, we show that a large number of interpretable controls can be defined by layer-wise perturbation along the principal directions. Moreover, we show that BigGAN can be controlled with layer-wise inputs in a StyleGAN-like manner. We show results on different GANs trained on various datasets, and demonstrate good qualitative matches to edit directions found through earlier supervised approaches.},
	urldate = {2022-12-16},
	publisher = {arXiv},
	author = {Härkönen, Erik and Hertzmann, Aaron and Lehtinen, Jaakko and Paris, Sylvain},
	month = dec,
	year = {2020},
	note = {arXiv:2004.02546 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Inversion, Computer Science - Graphics},
	annote = {Comment: Accepted to NeurIPS 2020},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/W35QV3L4/Härkönen et al. - 2020 - GANSpace Discovering Interpretable GAN Controls.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/TC3MLLQK/2004.html:text/html},
}

@article{vallez_diffeomorphic_2022,
	title = {Diffeomorphic transforms for data augmentation of highly variable shape and texture objects},
	volume = {219},
	issn = {01692607},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169260722001614},
	doi = {10.1016/j.cmpb.2022.106775},
	abstract = {Background and objective: Training a deep convolutional neural network (CNN) for automatic image classiﬁcation requires a large database with images of labeled samples. However, in some applications such as biology and medicine only a few experts can correctly categorize each sample. Experts are able to identify small changes in shape and texture which go unnoticed by untrained people, as well as distinguish between objects in the same class that present drastically different shapes and textures. This means that currently available databases are too small and not suitable to train deep learning models from scratch. To deal with this problem, data augmentation techniques are commonly used to increase the dataset size. However, typical data augmentation methods introduce artifacts or apply distortions to the original image, which instead of creating new realistic samples, obtain basic spatial variations of the original ones.
Methods: We propose a novel data augmentation procedure which generates new realistic samples, by combining two samples that belong to the same class. Although the idea behind the method described in this paper is to mimic the variations that diatoms experience in different stages of their life cycle, it has also been demonstrated in glomeruli and pollen identiﬁcation problems. This new data augmentation procedure is based on morphing and image registration methods that perform diffeomorphic transformations.
Results: The proposed technique achieves an increase in accuracy over existing techniques of 0.47\%, 1.47\%, and 0.23\% for diatom, glomeruli and pollen problems respectively.
Conclusions: For the Diatom dataset, the method is able to simulate the shape changes in different diatom life cycle stages, and thus, images generated resemble newly acquired samples with intermediate shapes. In fact, the other methods compared obtained worse results than those which were not using data augmentation. For the Glomeruli dataset, the method is able to add new samples with different shapes and degrees of sclerosis (through different textures). This is the case where our proposed DA method is more beneﬁcial, when objects highly differ in both shape and texture. Finally, for the Pollen dataset, since there are only small variations between samples in a few classes and this dataset has other features such as noise which are likely to beneﬁt other existing DA techniques, the method still shows an improvement of the results.},
	language = {en},
	urldate = {2022-12-16},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Vallez, Noelia and Bueno, Gloria and Deniz, Oscar and Blanco, Saul},
	month = jun,
	year = {2022},
	pages = {106775},
	file = {Vallez et al. - 2022 - Diffeomorphic transforms for data augmentation of .pdf:/home/mraillat/snap/zotero-snap/common/Zotero/storage/29ERHE8Y/Vallez et al. - 2022 - Diffeomorphic transforms for data augmentation of .pdf:application/pdf},
}

@misc{chen_infogan_2016,
	title = {{InfoGAN}: {Interpretable} {Representation} {Learning} by {Information} {Maximizing} {Generative} {Adversarial} {Nets}},
	shorttitle = {{InfoGAN}},
	url = {http://arxiv.org/abs/1606.03657},
	doi = {10.48550/arXiv.1606.03657},
	abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	month = jun,
	year = {2016},
	note = {arXiv:1606.03657 [cs, stat]},
	keywords = {GANs, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/8U8VX9FA/Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by .pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/UPFMIT7N/1606.html:text/html},
}

@misc{zhu_generative_2018,
	title = {Generative {Visual} {Manipulation} on the {Natural} {Image} {Manifold}},
	url = {http://arxiv.org/abs/1609.03552},
	doi = {10.48550/arXiv.1609.03552},
	abstract = {Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Zhu, Jun-Yan and Krähenbühl, Philipp and Shechtman, Eli and Efros, Alexei A.},
	month = dec,
	year = {2018},
	note = {arXiv:1609.03552 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Inversion},
	annote = {Comment: In European Conference on Computer Vision (ECCV 2016)},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/E6NC8P64/Zhu et al. - 2018 - Generative Visual Manipulation on the Natural Imag.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/PPTBRRM2/1609.html:text/html},
}

@misc{huh_transforming_2020,
	title = {Transforming and {Projecting} {Images} into {Class}-conditional {Generative} {Networks}},
	url = {http://arxiv.org/abs/2005.01703},
	doi = {10.48550/arXiv.2005.01703},
	abstract = {We present a method for projecting an input image into the space of a class-conditional generative neural network. We propose a method that optimizes for transformation to counteract the model biases in generative neural networks. Specifically, we demonstrate that one can solve for image translation, scale, and global color transformation, during the projection optimization to address the object-center bias and color bias of a Generative Adversarial Network. This projection process poses a difficult optimization problem, and purely gradient-based optimizations fail to find good solutions. We describe a hybrid optimization strategy that finds good projections by estimating transformations and class parameters. We show the effectiveness of our method on real images and further demonstrate how the corresponding projections lead to better editability of these images.},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Huh, Minyoung and Zhang, Richard and Zhu, Jun-Yan and Paris, Sylvain and Hertzmann, Aaron},
	month = aug,
	year = {2020},
	note = {arXiv:2005.01703 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Inversion},
	annote = {Comment: Accepted to ECCV2020 (oral)},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/FR5NCSGF/Huh et al. - 2020 - Transforming and Projecting Images into Class-cond.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/92JVWLXU/2005.html:text/html},
}

@misc{tov_designing_2021,
	title = {Designing an {Encoder} for {StyleGAN} {Image} {Manipulation}},
	url = {http://arxiv.org/abs/2102.02766},
	doi = {10.48550/arXiv.2102.02766},
	abstract = {Recently, there has been a surge of diverse methods for performing image editing by employing pre-trained unconditional generators. Applying these methods on real images, however, remains a challenge, as it necessarily requires the inversion of the images into their latent space. To successfully invert a real image, one needs to find a latent code that reconstructs the input image accurately, and more importantly, allows for its meaningful manipulation. In this paper, we carefully study the latent space of StyleGAN, the state-of-the-art unconditional generator. We identify and analyze the existence of a distortion-editability tradeoff and a distortion-perception tradeoff within the StyleGAN latent space. We then suggest two principles for designing encoders in a manner that allows one to control the proximity of the inversions to regions that StyleGAN was originally trained on. We present an encoder based on our two principles that is specifically designed for facilitating editing on real images by balancing these tradeoffs. By evaluating its performance qualitatively and quantitatively on numerous challenging domains, including cars and horses, we show that our inversion method, followed by common editing techniques, achieves superior real-image editing quality, with only a small reconstruction accuracy drop.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Tov, Omer and Alaluf, Yuval and Nitzan, Yotam and Patashnik, Or and Cohen-Or, Daniel},
	month = feb,
	year = {2021},
	note = {arXiv:2102.02766 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/EB96BW36/Tov et al. - 2021 - Designing an Encoder for StyleGAN Image Manipulati.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/LPRP8RBI/2102.html:text/html},
}

@misc{bai_high-fidelity_2022,
	title = {High-fidelity {GAN} {Inversion} with {Padding} {Space}},
	url = {http://arxiv.org/abs/2203.11105},
	doi = {10.48550/arXiv.2203.11105},
	abstract = {Inverting a Generative Adversarial Network (GAN) facilitates a wide range of image editing tasks using pre-trained generators. Existing methods typically employ the latent space of GANs as the inversion space yet observe the insufficient recovery of spatial details. In this work, we propose to involve the padding space of the generator to complement the latent space with spatial information. Concretely, we replace the constant padding (e.g., usually zeros) used in convolution layers with some instance-aware coefficients. In this way, the inductive bias assumed in the pre-trained model can be appropriately adapted to fit each individual image. Through learning a carefully designed encoder, we manage to improve the inversion quality both qualitatively and quantitatively, outperforming existing alternatives. We then demonstrate that such a space extension barely affects the native GAN manifold, hence we can still reuse the prior knowledge learned by GANs for various downstream applications. Beyond the editing tasks explored in prior arts, our approach allows a more flexible image manipulation, such as the separate control of face contour and facial details, and enables a novel editing manner where users can customize their own manipulations highly efficiently.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Bai, Qingyan and Xu, Yinghao and Zhu, Jiapeng and Xia, Weihao and Yang, Yujiu and Shen, Yujun},
	month = jul,
	year = {2022},
	note = {arXiv:2203.11105 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ECCV 2022 camera-ready; Project page: https://ezioby.github.io/padinv/; Code: https://github.com/EzioBy/padinv},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/ETGNRUSG/Bai et al. - 2022 - High-fidelity GAN Inversion with Padding Space.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/UPDRHGFI/2203.html:text/html},
}

@article{noauthor_notitle_nodate,
}

@misc{wu_stylespace_2020,
	title = {{StyleSpace} {Analysis}: {Disentangled} {Controls} for {StyleGAN} {Image} {Generation}},
	shorttitle = {{StyleSpace} {Analysis}},
	url = {http://arxiv.org/abs/2011.12799},
	doi = {10.48550/arXiv.2011.12799},
	abstract = {We explore and analyze the latent style space of StyleGAN2, a state-of-the-art architecture for image generation, using models pretrained on several different datasets. We first show that StyleSpace, the space of channel-wise style parameters, is significantly more disentangled than the other intermediate latent spaces explored by previous works. Next, we describe a method for discovering a large collection of style channels, each of which is shown to control a distinct visual attribute in a highly localized and disentangled manner. Third, we propose a simple method for identifying style channels that control a specific attribute, using a pretrained classifier or a small number of example images. Manipulation of visual attributes via these StyleSpace controls is shown to be better disentangled than via those proposed in previous works. To show this, we make use of a newly proposed Attribute Dependency metric. Finally, we demonstrate the applicability of StyleSpace controls to the manipulation of real images. Our findings pave the way to semantically meaningful and well-disentangled image manipulations via simple and intuitive interfaces.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Wu, Zongze and Lischinski, Dani and Shechtman, Eli},
	month = dec,
	year = {2020},
	note = {arXiv:2011.12799 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Graphics},
	annote = {Comment: 25 pages, 21 figures},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/VF5Q9VSF/Wu et al. - 2020 - StyleSpace Analysis Disentangled Controls for Sty.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/MCQJV62K/2011.html:text/html},
}

@misc{shen_interpreting_2020,
	title = {Interpreting the {Latent} {Space} of {GANs} for {Semantic} {Face} {Editing}},
	url = {http://arxiv.org/abs/1907.10786},
	doi = {10.48550/arXiv.1907.10786},
	abstract = {Despite the recent advance of Generative Adversarial Networks (GANs) in high-fidelity image synthesis, there lacks enough understanding of how GANs are able to map a latent code sampled from a random distribution to a photo-realistic image. Previous work assumes the latent space learned by GANs follows a distributed representation but observes the vector arithmetic phenomenon. In this work, we propose a novel framework, called InterFaceGAN, for semantic face editing by interpreting the latent semantics learned by GANs. In this framework, we conduct a detailed study on how different semantics are encoded in the latent space of GANs for face synthesis. We find that the latent code of well-trained generative models actually learns a disentangled representation after linear transformations. We explore the disentanglement between various semantics and manage to decouple some entangled semantics with subspace projection, leading to more precise control of facial attributes. Besides manipulating gender, age, expression, and the presence of eyeglasses, we can even vary the face pose as well as fix the artifacts accidentally generated by GAN models. The proposed method is further applied to achieve real image manipulation when combined with GAN inversion methods or some encoder-involved models. Extensive results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable facial attribute representation.},
	urldate = {2023-04-11},
	publisher = {arXiv},
	author = {Shen, Yujun and Gu, Jinjin and Tang, Xiaoou and Zhou, Bolei},
	month = mar,
	year = {2020},
	note = {arXiv:1907.10786 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR2020 camera-ready},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/LXZBV9S4/Shen et al. - 2020 - Interpreting the Latent Space of GANs for Semantic.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/9K6NYKMH/1907.html:text/html},
}

@misc{alaluf_third_2022,
	title = {Third {Time}'s the {Charm}? {Image} and {Video} {Editing} with {StyleGAN3}},
	shorttitle = {Third {Time}'s the {Charm}?},
	url = {http://arxiv.org/abs/2201.13433},
	doi = {10.48550/arXiv.2201.13433},
	abstract = {StyleGAN is arguably one of the most intriguing and well-studied generative models, demonstrating impressive performance in image generation, inversion, and manipulation. In this work, we explore the recent StyleGAN3 architecture, compare it to its predecessor, and investigate its unique advantages, as well as drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained on unaligned data, one can still use aligned data for training, without hindering the ability to generate unaligned imagery. Next, our analysis of the disentanglement of the different latent spaces of StyleGAN3 indicates that the commonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts, underscoring the benefits of using the StyleSpace for fine-grained editing. Considering image inversion, we observe that existing encoder-based techniques struggle when trained on unaligned data. We therefore propose an encoding scheme trained solely on aligned data, yet can still invert unaligned images. Finally, we introduce a novel video inversion and editing workflow that leverages the capabilities of a fine-tuned StyleGAN3 generator to reduce texture sticking and expand the field of view of the edited video.},
	urldate = {2023-04-10},
	publisher = {arXiv},
	author = {Alaluf, Yuval and Patashnik, Or and Wu, Zongze and Zamir, Asif and Shechtman, Eli and Lischinski, Dani and Cohen-Or, Daniel},
	month = jan,
	year = {2022},
	note = {arXiv:2201.13433 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Project page available at https://yuval-alaluf.github.io/stylegan3-editing/},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/CVM8SWIV/Alaluf et al. - 2022 - Third Time's the Charm Image and Video Editing wi.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/36868QQY/2201.html:text/html},
}

@misc{zhang_unreasonable_2018,
	title = {The {Unreasonable} {Effectiveness} of {Deep} {Features} as a {Perceptual} {Metric}},
	url = {http://arxiv.org/abs/1801.03924},
	doi = {10.48550/arXiv.1801.03924},
	abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
	urldate = {2023-04-09},
	publisher = {arXiv},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
	month = apr,
	year = {2018},
	note = {arXiv:1801.03924 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {Comment: Accepted to CVPR 2018; Code and data available at https://www.github.com/richzhang/PerceptualSimilarity},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/NF3N4DD6/Zhang et al. - 2018 - The Unreasonable Effectiveness of Deep Features as.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/NXXZSS4T/1801.html:text/html},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2023-04-09},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/TMYUK37Y/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/S4HWHVG4/1512.html:text/html},
}

@misc{richardson_encoding_2021,
	title = {Encoding in {Style}: a {StyleGAN} {Encoder} for {Image}-to-{Image} {Translation}},
	shorttitle = {Encoding in {Style}},
	url = {http://arxiv.org/abs/2008.00951},
	doi = {10.48550/arXiv.2008.00951},
	abstract = {We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard invert first, edit later methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain.},
	urldate = {2023-04-08},
	publisher = {arXiv},
	author = {Richardson, Elad and Alaluf, Yuval and Patashnik, Or and Nitzan, Yotam and Azar, Yaniv and Shapiro, Stav and Cohen-Or, Daniel},
	month = apr,
	year = {2021},
	note = {arXiv:2008.00951 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2021, project page available at https://eladrich.github.io/pixel2style2pixel/},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/MZLZ2G8Y/Richardson et al. - 2021 - Encoding in Style a StyleGAN Encoder for Image-to.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/B5IHEZTK/2008.html:text/html},
}

@inproceedings{blau_perception-distortion_2018,
	title = {The {Perception}-{Distortion} {Tradeoff}},
	url = {http://arxiv.org/abs/1711.06077},
	doi = {10.1109/CVPR.2018.00652},
	abstract = {Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms.},
	urldate = {2023-04-08},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Blau, Yochai and Michaeli, Tomer},
	month = jun,
	year = {2018},
	note = {arXiv:1711.06077 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {6228--6237},
	annote = {Comment: CVPR 2018 (long oral presentation), see talk at: https://youtu.be/\_aXbGqdEkjk?t=39m43s},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/46WNW8S4/Blau et Michaeli - 2018 - The Perception-Distortion Tradeoff.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/VMDTR9LX/1711.html:text/html},
}

@misc{abdal_image2stylegan_2019,
	title = {{Image2StyleGAN}: {How} to {Embed} {Images} {Into} the {StyleGAN} {Latent} {Space}?},
	shorttitle = {{Image2StyleGAN}},
	url = {http://arxiv.org/abs/1904.03189},
	doi = {10.48550/arXiv.1904.03189},
	abstract = {We propose an efficient algorithm to embed a given image into the latent space of StyleGAN. This embedding enables semantic image editing operations that can be applied to existing photographs. Taking the StyleGAN trained on the FFHQ dataset as an example, we show results for image morphing, style transfer, and expression transfer. Studying the results of the embedding algorithm provides valuable insights into the structure of the StyleGAN latent space. We propose a set of experiments to test what class of images can be embedded, how they are embedded, what latent space is suitable for embedding, and if the embedding is semantically meaningful.},
	urldate = {2023-04-08},
	publisher = {arXiv},
	author = {Abdal, Rameen and Qin, Yipeng and Wonka, Peter},
	month = sep,
	year = {2019},
	note = {arXiv:1904.03189 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted for oral presentation at ICCV 2019, "For videos visit https://youtu.be/RnTXLXw9o\_I , https://youtu.be/zJoYY2eHAF0 and https://youtu.be/bA893L-PjbI"},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/XGD4HB75/Abdal et al. - 2019 - Image2StyleGAN How to Embed Images Into the Style.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/3GG5B5Q8/1904.html:text/html},
}

@misc{roich_pivotal_2021,
	title = {Pivotal {Tuning} for {Latent}-based {Editing} of {Real} {Images}},
	url = {http://arxiv.org/abs/2106.05744},
	doi = {10.48550/arXiv.2106.05744},
	abstract = {Recently, a surge of advanced facial editing techniques have been proposed that leverage the generative power of a pre-trained StyleGAN. To successfully edit an image this way, one must first project (or invert) the image into the pre-trained generator's domain. As it turns out, however, StyleGAN's latent space induces an inherent tradeoff between distortion and editability, i.e. between maintaining the original appearance and convincingly altering some of its attributes. Practically, this means it is still challenging to apply ID-preserving facial latent-space editing to faces which are out of the generator's domain. In this paper, we present an approach to bridge this gap. Our technique slightly alters the generator, so that an out-of-domain image is faithfully mapped into an in-domain latent code. The key idea is pivotal tuning - a brief training process that preserves the editing quality of an in-domain latent region, while changing its portrayed identity and appearance. In Pivotal Tuning Inversion (PTI), an initial inverted latent code serves as a pivot, around which the generator is fined-tuned. At the same time, a regularization term keeps nearby identities intact, to locally contain the effect. This surgical training process ends up altering appearance features that represent mostly identity, without affecting editing capabilities. We validate our technique through inversion and editing metrics, and show preferable scores to state-of-the-art methods. We further qualitatively demonstrate our technique by applying advanced edits (such as pose, age, or expression) to numerous images of well-known and recognizable identities. Finally, we demonstrate resilience to harder cases, including heavy make-up, elaborate hairstyles and/or headwear, which otherwise could not have been successfully inverted and edited by state-of-the-art methods.},
	urldate = {2023-04-08},
	publisher = {arXiv},
	author = {Roich, Daniel and Mokady, Ron and Bermano, Amit H. and Cohen-Or, Daniel},
	month = jun,
	year = {2021},
	note = {arXiv:2106.05744 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/II27MIP3/Roich et al. - 2021 - Pivotal Tuning for Latent-based Editing of Real Im.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/2TXUPX59/2106.html:text/html},
}

@misc{johnson_perceptual_2016,
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	url = {http://arxiv.org/abs/1603.08155},
	doi = {10.48550/arXiv.1603.08155},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a {\textbackslash}emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing {\textbackslash}emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	urldate = {2023-04-08},
	publisher = {arXiv},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	month = mar,
	year = {2016},
	note = {arXiv:1603.08155 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/D3BY9Z52/Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/WBUKQKVH/1603.html:text/html},
}

@misc{thanh-tung_catastrophic_2020,
	title = {On {Catastrophic} {Forgetting} and {Mode} {Collapse} in {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1807.04015},
	abstract = {In this paper, we show that Generative Adversarial Networks (GANs) suffer from catastrophic forgetting even when they are trained to approximate a single target distribution. We show that GAN training is a continual learning problem in which the sequence of changing model distributions is the sequence of tasks to the discriminator. The level of mismatch between tasks in the sequence determines the level of forgetting. Catastrophic forgetting is interrelated to mode collapse and can make the training of GANs non-convergent. We investigate the landscape of the discriminator’s output in different variants of GANs and ﬁnd that when a GAN converges to a good equilibrium, real training datapoints are wide local maxima of the discriminator. We empirically show the relationship between the sharpness of local maxima and mode collapse and generalization in GANs. We show how catastrophic forgetting prevents the discriminator from making real datapoints local maxima, and thus causes non-convergence. Finally, we study methods for preventing catastrophic forgetting in GANs.},
	language = {en},
	urldate = {2023-04-08},
	publisher = {arXiv},
	author = {Thanh-Tung, Hoang and Tran, Truyen},
	month = mar,
	year = {2020},
	note = {arXiv:1807.04015 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: This is an extended version of our paper in ICML'18 Workshop on Theoretical Foundation and Applications of Deep Generative Models. Accepted to IJCNN 2020},
	file = {Thanh-Tung et Tran - 2020 - On Catastrophic Forgetting and Mode Collapse in Ge.pdf:/home/mraillat/snap/zotero-snap/common/Zotero/storage/GWQ4ZMAA/Thanh-Tung et Tran - 2020 - On Catastrophic Forgetting and Mode Collapse in Ge.pdf:application/pdf},
}

@misc{kodali_convergence_2017,
	title = {On {Convergence} and {Stability} of {GANs}},
	url = {http://arxiv.org/abs/1705.07215},
	abstract = {We propose studying GAN training dynamics as regret minimization, which is in contrast to the popular view that there is consistent minimization of a divergence between real and generated distributions. We analyze the convergence of GAN training from this new point of view to understand why mode collapse happens. We hypothesize the existence of undesirable local equilibria in this non-convex game to be responsible for mode collapse. We observe that these local equilibria often exhibit sharp gradients of the discriminator function around some real data points. We demonstrate that these degenerate local equilibria can be avoided with a gradient penalty scheme called DRAGAN. We show that DRAGAN enables faster training, achieves improved stability with fewer mode collapses, and leads to generator networks with better modeling performance across a variety of architectures and objective functions.},
	language = {en},
	urldate = {2023-04-08},
	publisher = {arXiv},
	author = {Kodali, Naveen and Abernethy, Jacob and Hays, James and Kira, Zsolt},
	month = dec,
	year = {2017},
	note = {arXiv:1705.07215 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
	annote = {Comment: Analysis of convergence and mode collapse by studying GAN training process as regret minimization. Some new results},
	file = {Kodali et al. - 2017 - On Convergence and Stability of GANs.pdf:/home/mraillat/snap/zotero-snap/common/Zotero/storage/Q8ESIUSX/Kodali et al. - 2017 - On Convergence and Stability of GANs.pdf:application/pdf},
}

@misc{bank_autoencoders_2021,
	title = {Autoencoders},
	url = {http://arxiv.org/abs/2003.05991},
	doi = {10.48550/arXiv.2003.05991},
	abstract = {An autoencoder is a specific type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the different types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.},
	urldate = {2023-04-08},
	publisher = {arXiv},
	author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
	month = apr,
	year = {2021},
	note = {arXiv:2003.05991 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Book chapter},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/4F94DLNJ/Bank et al. - 2021 - Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/DR88X35Y/2003.html:text/html},
}

@misc{heusel_gans_2018,
	title = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Local} {Nash} {Equilibrium}},
	url = {http://arxiv.org/abs/1706.08500},
	abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr{\textbackslash}'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	month = jan,
	year = {2018},
	note = {arXiv:1706.08500 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Implementations are available at: https://github.com/bioinf-jku/TTUR},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/DQIPXFRV/Heusel et al. - 2018 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/XZXJR3GQ/1706.html:text/html},
}

@misc{tancik_fourier_2020,
	title = {Fourier {Features} {Let} {Networks} {Learn} {High} {Frequency} {Functions} in {Low} {Dimensional} {Domains}},
	url = {http://arxiv.org/abs/2006.10739},
	doi = {10.48550/arXiv.2006.10739},
	abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
	urldate = {2023-04-12},
	publisher = {arXiv},
	author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
	month = jun,
	year = {2020},
	note = {arXiv:2006.10739 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Project page: https://people.eecs.berkeley.edu/{\textasciitilde}bmild/fourfeat/},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/SIMCHF7Z/Tancik et al. - 2020 - Fourier Features Let Networks Learn High Frequency.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/QWQW9YGW/2006.html:text/html},
}

@misc{mescheder_which_2018,
	title = {Which {Training} {Methods} for {GANs} do actually {Converge}?},
	url = {http://arxiv.org/abs/1801.04406},
	doi = {10.48550/arXiv.1801.04406},
	abstract = {Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
	month = jul,
	year = {2018},
	note = {arXiv:1801.04406 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
	annote = {Comment: conference},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/BNSEEUSS/Mescheder et al. - 2018 - Which Training Methods for GANs do actually Conver.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/JJTF67HP/1801.html:text/html},
}

@misc{chen_improved_2020,
	title = {Improved {Baselines} with {Momentum} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2003.04297},
	doi = {10.48550/arXiv.2003.04297},
	abstract = {Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
	month = mar,
	year = {2020},
	note = {arXiv:2003.04297 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report, 2 pages + references},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/64IEZ3XB/Chen et al. - 2020 - Improved Baselines with Momentum Contrastive Learn.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/ICBNIKIS/2003.html:text/html},
}

@misc{nunn_compound_2021,
	title = {Compound {Frechet} {Inception} {Distance} for {Quality} {Assessment} of {GAN} {Created} {Images}},
	url = {http://arxiv.org/abs/2106.08575},
	doi = {10.48550/arXiv.2106.08575},
	abstract = {Generative adversarial networks or GANs are a type of generative modeling framework. GANs involve a pair of neural networks engaged in a competition in iteratively creating fake data, indistinguishable from the real data. One notable application of GANs is developing fake human faces, also known as "deep fakes," due to the deep learning algorithms at the core of the GAN framework. Measuring the quality of the generated images is inherently subjective but attempts to objectify quality using standardized metrics have been made. One example of objective metrics is the Frechet Inception Distance (FID), which measures the difference between distributions of feature vectors for two separate datasets of images. There are situations that images with low perceptual qualities are not assigned appropriate FID scores. We propose to improve the robustness of the evaluation process by integrating lower-level features to cover a wider array of visual defects. Our proposed method integrates three levels of feature abstractions to evaluate the quality of generated images. Experimental evaluations show better performance of the proposed method for distorted images.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Nunn, Eric J. and Khadivi, Pejman and Samavi, Shadrokh},
	month = jun,
	year = {2021},
	note = {arXiv:2106.08575 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: 11 pages, 10 figures},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/XQQ97FDA/Nunn et al. - 2021 - Compound Frechet Inception Distance for Quality As.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/C27W24KW/2106.html:text/html},
}

@misc{heusel_gans_2018-1,
	title = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Local} {Nash} {Equilibrium}},
	url = {http://arxiv.org/abs/1706.08500},
	doi = {10.48550/arXiv.1706.08500},
	abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr{\textbackslash}'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	month = jan,
	year = {2018},
	note = {arXiv:1706.08500 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Implementations are available at: https://github.com/bioinf-jku/TTUR},
	file = {arXiv Fulltext PDF:/home/mraillat/snap/zotero-snap/common/Zotero/storage/B66HV7HW/Heusel et al. - 2018 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:application/pdf;arXiv.org Snapshot:/home/mraillat/snap/zotero-snap/common/Zotero/storage/2NP8IIRP/1706.html:text/html},
}
