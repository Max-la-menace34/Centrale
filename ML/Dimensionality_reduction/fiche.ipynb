{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionnality reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFECV, RFE\n",
    "from sklearn.feature_selection._base import SelectorMixin\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import numpy as np \n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity_scorer(clf, X=None, y=None, scale_coef=1.0):\n",
    "    \"\"\"\n",
    "    Evaluates the sparsity of the estimator within a pipeline\n",
    "    Can work with a LogisticRegression or a pipeline containing\n",
    "    a LogisticRegression\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        clf: a LogisticRegression or a Pipeline\n",
    "        X: ndarray (unused)\n",
    "        y: ndarray (unused)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        sparsity: float in [0., 1.]\n",
    "                  0. means all the dimensions are used,\n",
    "                  1. means no dimension are used\n",
    "    \"\"\"\n",
    "    if isinstance(clf, Pipeline):\n",
    "        clf_item = None\n",
    "        coef = scale_coef\n",
    "        for k, v in clf.named_steps.items():\n",
    "            if isinstance(v, SelectorMixin):\n",
    "                coef *= v.get_support().sum() / v.get_support().size\n",
    "            elif isinstance(v, LogisticRegression):\n",
    "                clf_item = v\n",
    "        if clf_item is None:\n",
    "            raise RuntimeError(\"Cannot estimate the sparsity of the pipeline\")\n",
    "        else:\n",
    "            return sparsity_scorer(clf_item, X, y, coef)\n",
    "    elif isinstance(clf, LogisticRegression):\n",
    "        non_null_coefs = clf.coef_.ravel() != 0\n",
    "        sparsity = 1.0 - non_null_coefs.sum() / non_null_coefs.size * scale_coef\n",
    "        return sparsity\n",
    "    else:\n",
    "        raise RuntimeError(f\"Cannot estimate the sparsity of a {type(clf)}\")\n",
    "\n",
    "\n",
    "class LinearPipeline(BaseEstimator):\n",
    "    def __init__(self, pipeline, clf_key):\n",
    "        super().__init__()\n",
    "        self.pipeline = pipeline\n",
    "        self.clf_key = clf_key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self.pipeline.fit(X, y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.pipeline.transform(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.pipeline.score(X, y)\n",
    "\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return self.pipeline.named_steps[self.clf_key].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy filter template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(ngram_range=(1, 2), min_df=2, num_folds=4, num_jobs=-1):\n",
    "    \"\"\"\n",
    "    A simple baseline model with bag-of-words on n-grams and\n",
    "    a logistic regression\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"===> [BEGIN] Simple model\")\n",
    "    \n",
    "    traindata, unsupdata, testdata = \"data_train\",\"data_unsup\",\"data_test\"; #load the data you need ! \n",
    "    pipe = Pipeline([('vectorizer', CountVectorizer(ngram_range=ngram_range,min_df=min_df)),\n",
    "        ('Max_abs', MaxAbsScaler()),\n",
    "        ('Logistic_reg', LogisticRegression(solver=\"liblinear\")),\n",
    "        ])\n",
    "    scores = cross_validate(pipe, traindata.data, traindata.target, cv=3,return_train_score=True)\n",
    "    pipe.fit(traindata.data,traindata.target)\n",
    "    print(scores['test_score'])\n",
    "\n",
    "    print(pipe.score(testdata.data, testdata.target))\n",
    "    vectorizer = pipe[\"vectorizer\"]\n",
    "    print(\"Saving the vocabulary in words_simple.txt\")\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    print(\"===> [END] Simple model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_filter(num_features, C, ngram_range, min_df, num_jobs=-1):\n",
    "    \"\"\"\n",
    "    Univariate filtering\n",
    "    \"\"\"\n",
    "    print(\"===> [BEGIN] Filter\")\n",
    "    traindata, unsupdata, testdata = \"data_train\",\"data_unsup\",\"data_test\"; #load the data you need !\n",
    "    chiiiiii_deux = chi2\n",
    "    pipe = Pipeline([('vectorizer', CountVectorizer(ngram_range=ngram_range,min_df=min_df,binary=True)),\n",
    "        (\"filter\", SelectKBest(chiiiiii_deux, k=num_features)),\n",
    "        ('Max_abs', MaxAbsScaler()),\n",
    "        ('Logistic_reg', LogisticRegression(solver=\"liblinear\")),\n",
    "        ])\n",
    "    pipe.fit(traindata.data,traindata.target)\n",
    "\n",
    "    acc_train = pipe.score(traindata.data, traindata.target)\n",
    "    acc_test = pipe.score(testdata.data, testdata.target)\n",
    "    \n",
    "    print(\"acc_train= \", acc_train,\"acc_test = \",acc_test)\n",
    "    \n",
    "    scores = cross_val_score(pipe,\n",
    "                             traindata.data, traindata.target,\n",
    "                             n_jobs=-1,\n",
    "                             verbose=0)\n",
    "    print(f\"Real risk by {scores.size}-fold CV : {scores.mean():.2} (+/- {scores.std():.2})\")\n",
    "    '''\n",
    "    Reminder : chi2 teste une hypothèse H0 et la valide ou non dans notre cas!\n",
    "    Ici on veut savoir si le mot permet de faire une bonne prédiction sur un message positif ou négatif\n",
    "    Si le mot donne de bonne prédiction alors on va le garder avec la fonction Select Kbest from sktlearn\n",
    "    Cela permet alors de réduir la dimension de notre entrée (nombres de features)\n",
    "    '''\n",
    "    counter = pipe[\"vectorizer\"]\n",
    "    chi2_filter = pipe[\"filter\"]\n",
    "    \n",
    "    selected_dims = chi2_filter.get_support() # Obteient un array avec True ou False pour chaque features \n",
    "    selected_scores = chi2_filter.scores_[selected_dims] # Ne garde que les Features où se situe True !\n",
    "    \n",
    "    sorted_idx = np.argsort(selected_scores)\n",
    "    selected_terms = np.array(counter.get_feature_names_out())[selected_dims] # Retrouve les mots qui ont été sélectionnés \n",
    "\n",
    "    print(selected_terms)\n",
    "    print(\"===> [Done] Filter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_embedded(ngram_range=(1, 2), min_df=2, C=0.5, n_folds=5, num_jobs=-1):\n",
    "    \"\"\"\n",
    "    LogisticRegression with L1 penalty\n",
    "    \"\"\"\n",
    "    print(\"===> [BEGIN] Embedded\")\n",
    "    traindata, unsupdata, testdata = \"data_train\",\"data_unsup\",\"data_test\"; #load the data you need !\n",
    "    pipe = Pipeline([('vectorizer', CountVectorizer(ngram_range=ngram_range,min_df=min_df,binary=True)),\n",
    "        ('Max_abs', MaxAbsScaler()),\n",
    "        ('Logistic_reg', LogisticRegression(C=C,penalty=\"l1\",solver=\"liblinear\")),\n",
    "        ])\n",
    "    print(\"Fitting the classifier\")\n",
    "    pipe.fit(traindata.data, traindata.target)\n",
    "\n",
    "    ''' \n",
    "     Also, we can introduce a measure of sparsity of your model as the fraction of \n",
    "     dimensions you kept from the original vocabulary and which can be computed \n",
    "     with the code we provide by calling the sparsity_scorer function.\n",
    "    '''\n",
    "    sparsity = sparsity_scorer(pipe)\n",
    "\n",
    "    \n",
    "    print(f\"Sparsity (fraction of zeros) : {100*sparsity:.2f}%\")\n",
    "\n",
    "    acc_train = pipe.score(traindata.data, traindata.target)\n",
    "    acc_test = pipe.score(testdata.data, testdata.target)\n",
    "    print(\n",
    "        f\"\"\"Train acc : {100*acc_train:.2f}%\n",
    "                  Test acc : {100*acc_test:.2f}%\"\"\"\n",
    "    )\n",
    "    # Extract and save the selected vocabulary\n",
    "    vocabulary = np.array(pipe.named_steps[\"vectorizer\"].get_feature_names_out())\n",
    "    selected_dims = pipe.named_steps[\"clf\"].coef_.ravel() != 0\n",
    "    selected_terms = vocabulary[selected_dims]\n",
    "    weights = pipe.named_steps[\"Logistic_reg\"].coef_.ravel()[selected_dims]\n",
    "    sorted_idx = np.argsort(weights)\n",
    "\n",
    "    print(f\"Original vocabulary size : {len(vocabulary)}\")\n",
    "    print(f\"Selected vocabulary size : {len(weights)}\")\n",
    "    print(selected_terms[sorted_idx], weights[sorted_idx])\n",
    "    print(\"===> [Done] Embedded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_wrapper(num_features=10000, step=10000, ngram=2, num_jobs=-1):\n",
    "    \"\"\"\n",
    "    Recursive feature elimination with logistic regression as estimator\n",
    "    \"\"\"\n",
    "    print(\"===> [BEGIN] imdb_wrapper\")\n",
    "    cache_name = \"imdb_wrapper.pkz\"\n",
    "    try:\n",
    "        X_train, y_train, X_test, y_test, vocabulary = utils.load_cache(\n",
    "            cache_name, [\"X_train\", \"y_train\", \"X_test\", \"y_test\", \"vocabulary\"]\n",
    "        )\n",
    "    except RuntimeError as err:\n",
    "        traindata, _, testdata = preprocess_imdb(num_jobs=num_jobs)\n",
    "\n",
    "        print(\"Vectorizing the data\")\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, ngram), min_df=2)\n",
    "        X_train = vectorizer.fit_transform(traindata.data)\n",
    "        y_train = traindata.target\n",
    "        X_test = vectorizer.transform(testdata.data)\n",
    "        y_test = testdata.target\n",
    "        vocabulary = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "        utils.save_cache(\n",
    "            cache_name,\n",
    "            {\n",
    "                \"X_train\": X_train,\n",
    "                \"y_train\": y_train,\n",
    "                \"X_test\": X_test,\n",
    "                \"y_test\": y_test,\n",
    "                \"vocabulary\": vocabulary,\n",
    "            },\n",
    "        )\n",
    "    print(f\"Original vocabulary size : {len(vocabulary)}\")\n",
    "\n",
    "    classifier = Pipeline(\n",
    "        [(\"scaler\", MaxAbsScaler()), (\"clf\", LogisticRegression(solver=\"liblinear\"))]\n",
    "    )\n",
    "    classifier = LinearPipeline(classifier, \"clf\")\n",
    "\n",
    "    selector = RFE(classifier, n_features_to_select=num_features, step=step, verbose=1)\n",
    "    print(\"Performing the recursive feature elimination\")\n",
    "    selector.fit(X_train, y_train)\n",
    "\n",
    "    # Compute its metrics\n",
    "    acc_train = selector.score(X_train, y_train)\n",
    "    acc_test = selector.score(X_test, y_test)\n",
    "    print(\n",
    "        f\"\"\"Train acc : {100*acc_train:.2f}%\n",
    "                  Test acc : {100*acc_test:.2f}%\"\"\"\n",
    "    )\n",
    "\n",
    "    selected_dims = selector.get_support()\n",
    "    selected_terms = vocabulary[selected_dims]\n",
    "    weights = selector.estimator_.pipeline.named_steps[\"clf\"].coef_.ravel()\n",
    "    sorted_idx = np.argsort(weights)\n",
    "\n",
    "    print(f\"Original vocabulary size : {len(vocabulary)}\")\n",
    "    print(f\"Selected vocabulary size : {len(weights)}\")\n",
    "    # ^^^^^^^^^\n",
    "    print(\"===> [END] imdb_wrapper\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
