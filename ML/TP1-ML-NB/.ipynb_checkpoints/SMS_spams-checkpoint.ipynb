{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&copy; CentraleSup√©lec.  \n",
    "For any question, contact the author at frederic.pennerath@centralesupelec.fr**\n",
    "\n",
    "<h1>\n",
    "<font size=\"30\">Spam versus ham</font>\n",
    "Naive Bayes for spam detection\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Despite the title, studying and testing *Naive Bayes* is only a secondary objective of this first labwork as this method is often too elementary and can be advantageously replaced by more sophisticated methods.\n",
    "This labwork has actually two more important objectives:  \n",
    "The first goal is to ensure, before going further, that you all have a basic knowledge and practice of the standard Python libraries & tools that are the most widely used in Data Science. They are:\n",
    "  - [numpy](https://numpy.org/) for numerical computation on multidimensional arrays (similar to Matlab).\n",
    "  - [matplotlib](https://matplotlib.org) for plotting various graphs.\n",
    "  - [scikit-learn](https://scikit-learn.org) as the standard machine learning Python library, that not only provides off-the-shelf ML methods but also standard generic tools & methods like cross validation, etc.\n",
    "  - [pandas](https://pandas.pydata.org) for manipulating tabular datasets similarly to a database (operations like indexing, filtering, sorting, join, etc), but in memory (so it does not suit processing of really big datasets).\n",
    "  - [seaborn](https://seaborn.pydata.org/), maybe less importantly than the others, is a statistical data visualization library extending ``matplotlib`` to easily provide various statistical views of your dataset (descriptive statistics).\n",
    "  - [jupyter](https://jupyter.org) is a service to write interactive notebooks in different languages (including Python). This is presently what are your are using. Jupyter is largely used by data scientists to prototype some code, iteratively explore data or build tutorials. Once your code works, you better make a real module from your notebooks.\n",
    "\n",
    "While all these libraries put all together are still not as rich as **R** (the preferred language of statisticians) for analyzing data, the Python ecosystem has the advantage to have many other libraries in all fields of application of computer science. For this reason, Python is more versatile than **R** and is the preferred choice in data science.\n",
    "\n",
    "\n",
    "The second goal is to illustrate some basic aspects of data analysis, including:\n",
    "  - Illustrating how a typical scenario handling data looks like (data cleansing, preprocessing & data analysis, model estimation, model evaluation then looping back to preprocessing again) based on scikit-learn, pandas & seaborn.\n",
    "  - Mentionning some beginner's traps.\n",
    "  - Presenting some practical aspects of classification: imbalanced classes and ROC AUC, cost sentitive classification, uncalibrated probabilities, etc\n",
    "  - How sometimes one must write some code to overcome the limitations of out-of-the-box algorithms.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject\n",
    "\n",
    "The labwork is a very classical application of Naive Bayes to address the problem of *spam detection*.\n",
    "*Spam messages* are massively sent and unwanted messages that one wishes to discard whereas *ham messages* are the remaining messages that should be kept.\n",
    "The term *Spam* and its relation with pork meat *ham* comes from a brand of canned cooked pork that is reused in a Monty Python sketch where all the meals in a restaurant tirelessly contain Spam.\n",
    "\n",
    "We consider here spam detection in textos (SMS) which are different from emails, as these text messages are much shorter, written in informal and sometimes slang or cryptic language with many abbreviations.\n",
    "Because this classification problem is relatively simple, Naive Bayes is a good choice as it is robust (no risk of overfitting even in small N large M problems like here), fast and simple to implement. See lectures to see how Naive Bayes works.\n",
    "\n",
    "Naive Bayes takes as input a set of monovalued features modelled by some univariate categorical or continuous distributions. \n",
    "Therefore to apply Naive Bayes to textual data, these texts have to be modelled as *bags of word*, i.e. as a large vector where each of its components represents a given word.\n",
    "We therefore work with a very high dimensional space whose size is the number of words in our vocabulary.\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "Of course for the labwork to be useful you must try to answer the questions by yourself.\n",
    "However if you are stuck or some question looks unclear, you might have a look at the provided solution that is\n",
    "the ``SMS_spams_solution.ipynb`` notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For those that have never used Jupyter, here are the main things to know:\n",
    "- A notebook is made of cells. Each cell can be a *code cell* containing runnable Python code, or a *Markdown cell* to contain textual infomation. The category of the selected cell is displayed in the dropdown menu on the Jupyter menu bar. Note that you can change this category.\n",
    "- If you want to edit the content of code or text cell, just double click on it.\n",
    "- To validate and run a code or Markdown cell, you must first select it and then press **CTRL + ENTER** to run it. The results (prints, etc) should appear below the cell. Pressing **SHIFT + ENTER** is almost the same but in addition, it jumps to the next cell.\n",
    "- Use the **Cell menu** to run all cells above or below.\n",
    "- Use the **Kernel menu** to restart from scratch if you entered some bad state or were getting lost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's start. First step is to import the classical modules by running the cell below.\n",
    "If you have installed correctly your packages (check the README file), you shouldn't get any error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# This magic function call automatically calls plt.show() for you when drawing some plots.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing: check, load and cleanse\n",
    "\n",
    "## Check the data\n",
    "\n",
    "Then the next thing to do is to look at the provided dataset ``SMS_spams.txt`` using command-line instructions like ``head``, ``tail`` or ``wc -l`` (on Unix systems at least) to understand its structure and size.\n",
    "\n",
    "*Note that you can directly call shell commands from Jupyter by prefixing with ``!`` like for instance ``!ls`` as shown below. However these commands are intended to check files quickly from a terminal and not as from a Jupyter notebook as they are not portable from one OS to another.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail SMS_spams.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l SMS_spams.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Once you understand the way the file is formatted, you have to build from it a in-memory representation (as the data are rather small) of the bags of words.\n",
    "\n",
    "We typically distinguish:\n",
    "* the classes (spam or ham) that we can representy concisely by a numpy 1D-array Y (with 0 for ham and 1 for spam)\n",
    "* the bags of words that can be represented by a numpy 2D-array X whose columns represent one given word and whose rows represent the different messages. \n",
    "\n",
    "However we'd better store X as a *Pandas dataframe*: dataframes index every column and row with a specific value, that can be a name (string). These indexes in turn allow fast access to rows and columns and efficient operations on it like filtering, joining, sorting, etc.\n",
    "For our application, dataframes are also very convenient here compared to numpy arrays as every column (and these are many here) can be mapped to a header or \"title name\" (in fact, the index entry), that is the word of the vocabulary mapped to the column.\n",
    "\n",
    "Below is a faked toy example of data to build a spam detector for baby sentences. It shows you how to create a numpy array, a Pandas dataframe and how to insert a 1D-array into an existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python list of considered words\n",
    "words = ['mum', 'dad', 'broumm', 'babbling']\n",
    "\n",
    "# This numpy array will contain the counts as produced by the scikit-learn vectorizer.\n",
    "counts = np.array([\n",
    "    [ 3, 0, 0, 1], # Baby 1\n",
    "    [ 0, 0, 0, 8], # Baby 2\n",
    "    [ 4, 0, 0, 0],\n",
    "    [ 1, 0, 0, 0],    \n",
    "    [ 0, 0, 3, 1]\n",
    "])\n",
    "\n",
    "# Class for every baby: 0 is ham (meaning: \"I need something\"), 1 is spam (meaning: \"Making noise is fun!\")\n",
    "Y = [0, 1, 0, 0, 1] \n",
    "\n",
    "# Builds a dataframe with word features\n",
    "X = pd.DataFrame(data = counts, columns = words)\n",
    "\n",
    "# Add the class as the first column\n",
    "X.insert(0, '#class#', Y)\n",
    "\n",
    "# Print the heading lines\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While ``pandas`` supports many load functions for various formats (like ``read_csv``, ``read_json``, ``read_excel``, etc), the idea here is to preprocess directly the file by reading every line (this is very easy in Python) and convert it into a bag of words.\n",
    "\n",
    "Because this is a specific treatment, the best solution is to develop your own function to load the dataset and vectorize it. \n",
    "To do so, you can use the scikit-learn object ``CountVectorizer`` as illustrated [here](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction).  \n",
    "\n",
    "**Question:**  \n",
    "Try to implement by yourself the Python function ``vectorize_dataset(filename)`` provided below, that:\n",
    "* reads the lines of the file one by one and feed a ``CountVectorizer`` with them. In order to reduce the number of features, we will drop non-informative words, called *stop words*, by passing argument ``stop_words='english'`` to the vectorizer constructor.\n",
    "* outputs a dataframe similar to the one of the baby example along with the array of messages.\n",
    "\n",
    "Help yourself by using GIYF (Google is your friend), tutorials, FAQ like [https://stackoverflow.com/](https://stackoverflow.com/), official documentation website [docs.python.org](https://docs.python.org/), etc.\n",
    "\n",
    "If you are beginner in Python, analyze the ``read_dataset`` function provided below that parses line by line the dataset to print the class and the length of every message.\n",
    "By adapting this function, introducing the vectorizer and following the baby example to produce the dataframe, you should be able to make it.\n",
    "\n",
    "Whether you choose the hard or the easy way, don't forget to test seriously before going further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regular expressions\n",
    "import re\n",
    "\n",
    "def read_dataset(filename):\n",
    "\n",
    "    # Compile the regular expression once and for all\n",
    "    expr = re.compile('\\s+')\n",
    "    \n",
    "    # Read the file line by line\n",
    "    with open(filename) as f:\n",
    "        for i,line in enumerate(f):\n",
    "            \n",
    "            # Separate class from textual words\n",
    "            splits = expr.split(line, 1)\n",
    "            \n",
    "            # Check the line structure\n",
    "            if(len(splits) < 2):\n",
    "                print(f'Bad syntax for {i+1}th line: \"{line}\"')\n",
    "            else:                \n",
    "                target, msg = splits\n",
    "                print(f\"{target:4} | {len(msg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dataset('SMS_spams.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import regular expressions and vectorizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def vectorize_dataset(filename):\n",
    "    counts = None # Pandas dataframe containing classes and word counts (see toy example)\n",
    "    messages = [] # List of text messages\n",
    "    \n",
    "    # Complete code here\n",
    "    \n",
    "    return counts, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, messages = vectorize_dataset('SMS_spams.txt')\n",
    "print(f\"Number of words = {len(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanse the data\n",
    "\n",
    "One you have loaded your data into a Pandas dataframe, the next step is to *cleanse the data*, i.e. to delete or to fix all entries or columns that are obviously wrong or unexploitable.\n",
    "\n",
    "A typical beginner's mistake made at this stage is to over-zealously modify the original data set.\n",
    "This introduces bias into the data and eventually does more harm than good.\n",
    "Here is an example of deleting \"empty\" entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Empty\" messages. Delete or not delete?\n",
    "\n",
    "We want to see if there exists \"empty\" messages containing only stop words or unrecognized words.\n",
    "\n",
    "**Question:**  \n",
    "Extract the row index from the Pandas dataframe that correspond to messages containing no keywords, only stopwords (i.e. all its word counts are zero).\n",
    "\n",
    "*This is an exercise for manipulating Python lists, numpy arrays and Pandas dataframes.\n",
    "Don't forget to avoid as much as possible explicit loops written in Python as they are slow. It is much faster to replace them by calls to **Numpy** or **Pandas** functions that are implemented in optimized C/C++.\n",
    "You can use function ``print_messages`` below to display the messages of the selected index.* \n",
    "\n",
    "*Don't spend too much time on this question. If you don't find, check the proposed solution (only one way among many to do it).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_messages(indexes):\n",
    "    classes = ['ham', 'spam']\n",
    "    rows = df.iloc[indexes,:]\n",
    "    for i, cat in zip(indexes, rows['#class#']):\n",
    "        print(classes[cat], \"|\", messages[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the indexes of messages containing only stopwords\n",
    "\n",
    "indexes = None\n",
    "\n",
    "# Now print the results\n",
    "print_messages(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "How should we manage these lines without keywords? Should we drop them? Or add more words to the vocabulary?\n",
    "Write some special bypassing code treatment?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-English words. Delete or not delete?\n",
    "\n",
    "It also seems many words are not English vocabulary, like abbreviations, smileys, words with special characters, etc. What we should do about them? Should we delete messages containing these unrecognized words? \n",
    "Clearly no, for the same reason as above.\n",
    "\n",
    "But should we remove every column which does not match an English word? Or should we keep it as it is?\n",
    "\n",
    "There is no simple answer. This really depends on the meaning of these special words.\n",
    "Let's see how many they are and how they look like.\n",
    "\n",
    "**Question:**  \n",
    "Print the list of words that contain at least one non alphabetical character.\n",
    "\n",
    "*To do this, you will need to access to the dataframe column index, given by the ``columns`` attribute.\n",
    "You might also take advantage of the method ``s.isalpha()`` that returns a boolean that is true if string ``s`` only contains alphabetical characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many non alphabetical words contain numbers. These numbers can carry some information (phone numbers either free or pay phone numbers, or other words like \"nokia6600\", both suggesting spam) or not (like words \"10\", \"8am\", \"2nd\", etc).\n",
    "\n",
    "The first category is something to exploit as an opportunity to do some *feature engineering*, that is, one can inject application domain knowledge to improve classifier's performance, by building new informative features that will indicate whether messages contain relevant word categories:\n",
    "- phone numbers, either free, pay phone or private numbers\n",
    "- proper nouns of persons, companies, countries, regions, cities, etc\n",
    "\n",
    "The second category is seemingly harmless, but is in fact very harmful if left unattended.\n",
    "This is because these words are both noisy (i.e. independent of the class) and very rare.\n",
    "For example, the number of messages containing 177 may be 1 or 2. Therefore, estimating the probability ``P(\"message contains 177\" | \"message is a spam (or a ham)\")`` using the default *MLE* estimator is nonsense.\n",
    "\n",
    "To circumvent this problem, we have seen during the lectures how to replace MLE by a regularized *MAP estimator* or by *Bayesian inference* which is even better. Both approaches amount to assume a *Beta prior* over probability of observing a given word in a spam or a ham message. The chosen Beta prior $Beta(\\alpha,\\beta)$ will typically promote a uniform distribution: for instance taking $\\alpha = \\beta = 4$ will assume 3 faked spams and 3 faked hams for every word). This would be a sound option.\n",
    "\n",
    "However we already know that most raw numbers are not informative with respect to the class.\n",
    "So it is better to just delete these words, and by the way, save some computational time by reducing substantially the number of columns.\n",
    "\n",
    "**Question:**  \n",
    "Remove the columns from the dataframe corresponding to non-alphabetical words.\n",
    "To drop columns, use the ``drop`` pandas method (check the online documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the data\n",
    "\n",
    "It's now time to look a little more closely the data and do some more statistical *sanity checks*:\n",
    "- Look at target distribution (how many classes, how balanced they are, etc)\n",
    "- Look at histograms of categorical & moments of numerical features (e.g use ``pd.describes()``)\n",
    "- Check for outliers, duplicates, etc\n",
    "- Remove useless features (columns with a lot of missing values or constant, etc) or remaining overspecific features (ids, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicated messages. Delete or not delete?\n",
    "\n",
    "Let's first look at duplicates using pandas method ``duplicated``. \n",
    "\n",
    "**Question:**  \n",
    "Are there many of them? What shoud we do with these? Remove or keep duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "It depends but like for messages with only stop words, the answer should be the result of some reflexion. \n",
    "\n",
    "Duplicates here are not artifacts (like redundant entries in a database because a sample has been collected multiple times).\n",
    "Here they are real duplicates, that is, identical messages sent several times.\n",
    "A duplicate is therefore more likely a spam. This is an interesting piece of information we should try to exploit.\n",
    "\n",
    "However if we don't add any extra feature, we won't have access to this information when processing message since, when our spam detector receives a new message, it does not know if this is a duplicate as it does not archive the history, i.e. all previous messages.\n",
    "In that case, keeping duplicates might be a bad idea as it might just increase the influence of some very duplicated messages in the training (and even test) dataset that won't occur in operation, on the long term (spams have generally a short life).\n",
    "\n",
    "If we decide to keep the history, then we can add a new feature to a message, that is the number of times identical or very similar messages have been previously received.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep it simple, one choose the easy way: \n",
    "\n",
    "**Question:**  \n",
    "Remove the duplicates using pandas method ``drop_duplicates()``. Check the size of your new dataset.\n",
    "Then look at the class distribution using method ``value_counts()`` of pandas series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check now the distribution of words.\n",
    "\n",
    "**Question:**  \n",
    "Plot the distribution of the number of occurrences of words using the dataframe method ``sum()``, the numpy function ``np.histogram()`` and the matplotlib function ``plt.bar()``.\n",
    "What do you conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Zipf's law\n",
    "\n",
    "**The next question is subsidiary and should be done at the end of the labwork, only if you have time.**\n",
    "\n",
    "The [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) is a *heavy-tailed distribution* that models particularly well the distribution of the frequency of words in many natural languages, since in most languages, very few words have a high frequency and many words have a very low frequency.\n",
    "\n",
    "The Zipf's law considers the relative frequencies $f(w)$ of every word $w$ observed in a large text corpus. Assuming words $(w_i)_{1\\leq i \\leq N}$ are sorted in decreasing order of their frequency, the Zipf's law states that \n",
    "$$f(w_i) \\approx \\frac{1}{K\\,i^s}$$\n",
    "for some $s > 0$ and some normalization factor $K$ such that the frequencies sum to one.\n",
    "\n",
    "**Question:**  \n",
    "Compare the distribution of words in textos with Zipf's law. To this end, find the optimal values for $K$ and $s$ such that the corresponding Zipf's law best matches $i \\mapsto f(w_i)$. \n",
    "\n",
    "**Indication:**  \n",
    "Taking the logarithm of the expression, we get a linear expression\n",
    "$$\\log f(w_i) \\approx - \\log(K) - s \\, \\log(i)$$\n",
    "We can thus determine the coefficients $\\log(K)$ and $s$ using an ordinary linear regression (using `stats.linregress`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data for scikit-learn\n",
    "\n",
    "Last preprocessing step before applying Naive Bayes is to transform data so that they can be processed as inputs by ``scikit-learn`` methods.\n",
    "On this aspect, ``scikit-learn`` appears not to be very flexible.\n",
    "In particular many methods cannot directly process categorical variables. \n",
    "These variables must first be converted to boolean vectors using *one-hot encoding*.\n",
    "Note that this is not anymore the case for Naive Bayes that supports such distribution now.\n",
    "\n",
    "But in general when working with ``scikit-learn``, you must:\n",
    "1. Encode the target class feature as consecutive integers (using class ``LabelEncoder`` if needed)\n",
    "2. Depending on the method you might need,\n",
    "  * Separate the categorical and numerical features,\n",
    "  * One-hot encode categorical features  \n",
    "3. Extract classes into a numpy 1D-array (denoted usually Y) and features as a 2D-array (denoted usually X)\n",
    "\n",
    "Hopefully here classes and features can be processed as binary variables: \n",
    "While the value of an input feature is the number of occurrences of the word it represents in a document, the implementation of Bernoulli Naive Bayes in scikit-learn interprets such features as binary: 1 if the count is non null, 0 otherwise. As a consequence, steps 1 and 2 are useless ; you just need to do step 3 as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['#class#'].values\n",
    "X = df.iloc[:,1:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Naive Bayes\n",
    "\n",
    "Scikit-learn's implementations of NB suffers from several limitations:\n",
    "- Only normal distribution is supported for continuous variables.\n",
    "- And the worst thing: **All features are assumed to be of the same family (either Bernoulli, multinomial or Gaussian)**.\n",
    "\n",
    "So let's see what can be achieved given these restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple cross validation\n",
    "\n",
    "**Accuracy** is the standard classification score.\n",
    "However in presence of imbalanced classes, the **area under the ROC curve** (ROC AUC) is more appropriate to assess the classifier robustness (see lecture on this topic).\n",
    "Finally in order to take into account the higher cost of false positive (spammed ham) compared to false negative (undeleted spam), one proposes the **average cost** as a third score. \n",
    "\n",
    "*scikit-learn* expects to instantiate a classifier object. When passed to the cross validation (thanks to function ``cross_validate``), a clone of this object (not the object itself) is trained on every fold to avoid any cross-border effect.\n",
    "In the following we will use the ``BernoulliNB`` class that can be straight-forwardly applied to our binary features. See its [documentation](https://scikit-learn.org/stable/modules/naive_bayes.html). In particular the classifier constructor accepts as argument an argument ``alpha`` that is a pseudocount (i.e ``alpha`` is equal to $\\alpha - 1$ where $\\alpha$ is the hyperparameter of the symmetrical Beta distribution $Beta(\\alpha,\\alpha)$ used as a prior for the Bernoulli probabilities).\n",
    "\n",
    "**Question:**  \n",
    "Complete the code below to evaluate the method on our data, according to accuracy and AUC (see the ``scoring`` argument of function ``cross_validate``). Save the results in a pandas dataframe as shown on the code snippet.\n",
    "What's your conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Create a BernoulliNB classifier \n",
    "\n",
    "# Cross-validate it, computing accuracy and auc on every fold\n",
    "\n",
    "# Compute the average accuracy and auc\n",
    "\n",
    "# Save results in a dataframe, using line below.\n",
    "scores = pd.DataFrame(data={'classifier': ['Bernoulli NB'], 'acc': [acc], 'auc': [auc]}, columns=[ 'classifier', 'acc', 'auc'])\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poor probability calibration of Naive Bayes \n",
    "\n",
    "Without further optimization, Naive Bayes already produces an excellent accuracy and AUC.\n",
    "However this does not tell us whether the class probabilities produced by Naive Bayes are well calibrated.\n",
    "\n",
    "To do so, we won't use cross validation here but instead we will split the dataset in two equal parts, one for training, the other for testing (evaluation). *scikit-learn* provides the function ``train_test_split`` for this purpose.\n",
    "\n",
    "**Question:**  \n",
    "Create a brand new classifier (to avoid cross-border effect) and train it on the first half using the ``fit`` method. Then output class probabilities on the test dataset using the ``predict_proba`` method and display the distribution of the probability for messages to be a spam. What do you conclude?\n",
    "\n",
    "One can use the visualization library *Seaborn*, in particular its ``displot()`` function (one recommends here to use the option ``kind='kde', bw_adjust=0.1``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train and test dataset using train_test_split\n",
    "\n",
    "# Fit a BernoulliNB model on the train dataset\n",
    "\n",
    "# Output class probabilities on the test dataset using method predict_proba()\n",
    "\n",
    "# Call seaborn displot on the probability vector of class spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration of output distributions\n",
    "\n",
    "Imagine a binary classifier that is evaluated on a test dataset of $n$ samples. For every sample $i \\in \\{1 \\dots n\\}$, it outputs a probability $p_i$ that this sample is a member of class 1.\n",
    "Let us consider some interval of probability, like for instance interval $I = [0.5, 0.7]$.\n",
    "Let us define the number $n_I$ of samples whose probability $p_i$ is within interval $I$. Among these $n_I$ samples, let denote $n_I^1 \\leq n_I$ the number of positive examples. We then expect that fraction $\\frac{n_I^1}{n_I}$ will be roughly equal to $0.6$ and clearly between 0.5 and 0.7. Same for any other interval containing sufficiently many samples to make a reliable estimation. If this is true, we say that the classifier is *well calibrated*.\n",
    "\n",
    "However this is rarely the case. For instance, we just saw that Naive Bayes outputs extreme probabilities close to 0 or 1. If such a classifier was well calibrated, this would imply an accuracy of 1 but this is obviously wrong.\n",
    "\n",
    "### Calibration plot\n",
    "\n",
    "One way to check whether a classifier is well calibrated, is to plot a *calibration plot*: it consists in partitioning regularly interval $[0,1]$ into $k$ subintervals $I_i = [(i-1)/k, i/k]$ and then plotting for each subinterval $I_i$ the ratio $n_{I_i} / n$. We except the plot to be close to the main diagonal.\n",
    "\n",
    "**Question:**  \n",
    "Use the scikit-learn function ``sklearn.calibration.calibration_curve`` to draw the calibration plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Call calibration_curve function on the test dataset using 10 bins (intervals)\n",
    "\n",
    "# Plot the result using matplotlib plot function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Interpret this plot by running the code below. Why the classifier is badly calibrated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spam_prob(probs):\n",
    "    '''\n",
    "    Plots the histogram of the spam probability with the class (colored ticks on the x-axis) mapped to every example\n",
    "    '''\n",
    "    Pspam = pd.DataFrame({'class': Y_test, 'Pspam': probs[:,1]})\n",
    "    sns.displot(data=Pspam, x='Pspam', bins=20, hue='class', rug=True, multiple='stack')\n",
    "\n",
    "# We assume probs is the numpy array returned by the predict_proba() method\n",
    "plot_spam_prob(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the calibration plot has a poor quality. Because intermediate probabilities (i.e not equal to 0 or 1) are mostly ambiguous spams (spams that look like hams), the calibration plot is clearly above the diagonal.\n",
    "However we see here another problem: many spams are classified as hams with a very low probability $P(Y=\\text{spam}\\,|\\,X)$. This will be the main source of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier calibration\n",
    "\n",
    "Calibration is the process of adjusting the output distributions of a badly calibrated model (a classifier or a regressor) in order for the resulting probabilities to be closer to the true probabilities.\n",
    "Different calibration methods are implemented in scikit-learn (isotonic regression or fitting a sigmoid).\n",
    "\n",
    "**Question:**  \n",
    "Complete the code below by using class ``sklearn.calibration.CalibratedClassifierCV`` to calibrate your classifier, using the `isotonic` method. Compare the calibration plots of both uncalibrated and calibrated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Ideal calibration curve\n",
    "plt.plot([0,1],[0,1], 'k--', label='Expected')\n",
    "\n",
    "# Plot the calibration curve for the uncalibrated Naive Bayes\n",
    "clf1 = BernoulliNB(alpha=1)\n",
    "clf1.fit(X_train, Y_train)\n",
    "probs1 = clf1.predict_proba(X_test)\n",
    "\n",
    "frac_pos, mean_probs =  calibration_curve(Y_test, probs1[:,1], n_bins=10)\n",
    "plt.plot(mean_probs, frac_pos, 'r-', label='Uncalibrated')\n",
    "\n",
    "# Plot the calibration curve for the calibrated Naive Bayes\n",
    "\n",
    "# Complete the code from here\n",
    "\n",
    "# First create a CalibratedClassifierCV based on Naive Bayes (see scikit-learn documentation)\n",
    "\n",
    "# Then fit it on train and estimate probabilities of spam on test\n",
    "\n",
    "# Finally plot the result on top of the previous plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves\n",
    "\n",
    "ROC curves are a tool to represent graphically the behaviour and performance of a binary classifier (see lecture notes) independently of the class ratios in the dataset. *Scikit-learn* provides function ``plot_roc_curve`` to easily plot the roc curve estimated on a test dataset.\n",
    "\n",
    "**Question:**  \n",
    "Compare the roc curves of calibrated and uncalibrated Naive Bayes. Are these curves good? Why there is no much difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# Split data into a train and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.6)\n",
    "\n",
    "# For each classifier (uncalibrated and calibrated Bernoulli NB) trained on the train dataset,\n",
    "# plot the ROC curve calling plot_roc_curbe on the test dataset\n",
    "\n",
    "clfs = {\n",
    "    'Bernoulli NB': BernoulliNB(alpha=1),    \n",
    "    'Calib. Bernoulli NB': CalibratedClassifierCV(BernoulliNB(alpha=1), cv=10, method='isotonic'),\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for name,clf in clfs.items():\n",
    "    clf.fit(X_train, Y_train)\n",
    "    plot_roc_curve(clf, X_test, Y_test, name=name, ax=ax) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no much difference as the calibration has mostly impacted the very few spam messages whose uncalibrated probability was between 0 and 1. The main problem remains the spams with a low estimated probability to be a spam.\n",
    "The calibration helps to fix the distribution of probabilities but cannot make miracles by separating badly classified examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some disgression about feature selection\n",
    "\n",
    "Another application of Naive Bayes is to score features according to their predictive power.\n",
    "It can be used for feature selection or just to have some insights about the problem.\n",
    "\n",
    "The discriminant power of every feature is expressed by their *log-odd* defined as:\n",
    "\n",
    "$$\n",
    "\\DeclareMathOperator{\\logodd}{log-odd}\n",
    "\\logodd(X) = \\log \\left(\\frac{P(Y=1 | X=1)}{P(Y=0| X=1)}\\right) = \\log\\left(P(Y=1 | X=1)\\right) - \\log\\left(P(Y=0 | X=1)\\right) \n",
    "$$\n",
    "These log-odds can be computed from the learnt parameters (applying Bayes'rule).\n",
    "The log-odd of a word $X$ is large (resp. small) if $X$ is characteristic of class $1$ (resp. $0$) and not of class $0$ (resp. $1$).\n",
    "\n",
    "Once  a  scikit Bernoulli classifier has learnt its parameters from some data (using the ``fit`` method), it provides the log-odds thanks to its ``feature_log_prob_`` attribute.\n",
    "\n",
    "**Question:**  \n",
    "Look at the attribute `feature_log_prob` (shape and meaning) of a Naive Bayes model trained on the whole dataset.\n",
    "Compute from it the log-odds and display the 20 top words characteristic that are most representative of spams and hams respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a NB model on the whole dataset\n",
    "\n",
    "# Look at feature_log_prob_ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the log-odds from the log_prob_ array\n",
    "logodds = None\n",
    "\n",
    "# Rank the features using np.argsort\n",
    "rankedFeatures = np.flip(np.argsort(logodds))\n",
    "keywords = df.columns[1:]\n",
    "\n",
    "# Create a dataframe containing the names of ranked features and their log-odd\n",
    "rankedKeyWords = pd.DataFrame(data=logodds[rankedFeatures], index=keywords[rankedFeatures], columns=['log-odd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top-K keywords using the Dataframe head method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now look at characteristic words of ham:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost sensitive classification\n",
    "\n",
    "Naive Bayes provides as outputs class distributions, even if they are poorly calibrated.\n",
    "We can therefore design from that a cost sensitive classifier (see lectures).\n",
    "\n",
    "To evaluate it, one first needs to consider a third additional measure that is the average cost.\n",
    "In the following, one will assume that a false spam has a cost $c_{fs}$ of 1 while a false ham has a cost $c_{fh}$ of 0.1.\n",
    "This way we expect an average cost between 0 and 1.\n",
    "To add this home-made scoring function, we will use functions ``make_scorer`` and ``confusion_matrix``. See documentation [make_scorer](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) to understand how to use it.\n",
    "\n",
    "To make our classifier cost sensitive, one has to produce as output, the class that minimizes the average cost according to the class distribution it predicts, given some input. \n",
    "\n",
    "**Question:**  \n",
    "For some message $X$, let denote $\\hat{y}$ the class ham or spam that provides the smallest expected loss $L(Y,\\hat{y})$ when $Y$ follows the distribution $P(Y\\,|\\,X,\\theta)$ output by the classifier.\n",
    "Determine the mathematical condition to decide whether $\\hat{y}=0$ (ham) or $\\hat{y}=1$ (spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to make our classifier cost-sensitive.\n",
    "The function `min_expected_cost()` computes the expected cost of a classifier.\n",
    "This function is registered as a new \"scorer\" to the cross validation procedure using the *Scikit-learn* function ``make_scorer``.\n",
    "To work function `min_expected_cost()` needs to call the function `choose_best_class()` that decides which class is the best given the probability of a message to be a spam.\n",
    "\n",
    "**Question:**  \n",
    "Once you have completed the body of function `choose_best_class()`, run the code to evaluate this new (negative) score. Explain the (small) difference of expected cost between both tested methods. Note that from now, we will use the provided class `ScoreBoard` to register scores of the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# False spam is assumed to be 10 times more costly than false ham.\n",
    "costFalseSpam, costFalseHam = 1, 0.1\n",
    "\n",
    "# Decision function\n",
    "def choose_best_class(Y_pred):\n",
    "    '''\n",
    "    Returns a 1D boolean array Y_best of the same dimension of Y_pred \n",
    "    containing the decision to take for each example (i.e true for spam, false for ham)\n",
    "    \n",
    "    - Y_pred is the vector of probabilities of the positive class (spam) for one given fold\n",
    "    '''\n",
    "   \n",
    "    # Create a 1D boolean array Y_best that contains the best choices for all examples, given Y_pred\n",
    "    \n",
    "    return Y_best\n",
    "\n",
    "# Cost evaluation function\n",
    "def min_expected_cost(Y, Y_pred):\n",
    "    # Y is the vector of the true classes for one given fold\n",
    "    # Y_pred is the vector of probabilities of the positive class (spam) for one given fold\n",
    "    \n",
    "    # Choose the best classes that minimize the expected loss\n",
    "    Y_best = choose_best_class(Y_pred)\n",
    "\n",
    "    # Compute the matrix of confusion and the number of errors of type I and II\n",
    "    C = confusion_matrix(Y, Y_best.astype(int))\n",
    "    nFalseSpam = C[0,1]\n",
    "    nFalseHam = C[1,0]\n",
    "    \n",
    "    # Compute the number of samples and the minimal expected cost\n",
    "    n = np.sum(C)\n",
    "    min_cost = (nFalseSpam * costFalseSpam + nFalseHam * costFalseHam) / n\n",
    "    return min_cost\n",
    "\n",
    "# Builds the score and adds it to the list of measures\n",
    "scorer = make_scorer(min_expected_cost, needs_proba=True)\n",
    "scoring_functions = {'acc' : 'accuracy', 'auc': 'roc_auc', 'cost': scorer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreBoard:\n",
    "    def __init__(self, scoring_functions={'acc' : 'accuracy', 'auc': 'roc_auc'}, n_folds=10):\n",
    "        self.scoring_functions = scoring_functions\n",
    "        self.score_names = list(scoring_functions.keys())\n",
    "        self.scores = pd.DataFrame(columns= [ 'classifier' ] + self.score_names)\n",
    "        self.n_folds = n_folds\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return self.scores._repr_html_()\n",
    "    \n",
    "    def test(self, name, clf, X, Y):\n",
    "        res  = cross_validate(clf, X, Y, cv=self.n_folds, scoring=self.scoring_functions)\n",
    "\n",
    "        entry = { score_name: np.mean(res[f'test_{score_name}']) for score_name in self.score_names }\n",
    "        entry['classifier'] = name           \n",
    "        self.scores = self.scores.append(entry, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a scoreboard\n",
    "scores = ScoreBoard(scoring_functions)\n",
    "\n",
    "# Then evaluate the uncalibrated NB\n",
    "clf = BernoulliNB(alpha=1)\n",
    "scores.test('Bernoulli NB', clf, X, Y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now evaluate the calibrated NB (takes time)\n",
    "clf = CalibratedClassifierCV(BernoulliNB(alpha=1), cv=10, method='isotonic')\n",
    "scores.test('Calibrated Bernoulli NB', clf, X, Y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a slight different of cost that can be explained as the expected cost is sensitive to probabilities. Thus better calibrated probabilities will make the expected cost decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "\n",
    "Instead of using Bernoulli Naive Bayes, one can use *multinomial Naive Bayes* (see lecture).\n",
    "The difference is that the model likelihood takes into account the number of occurrences of a word in documents.\n",
    "\n",
    "**Question:**  \n",
    "Test multinomial Naive Bayes using class ``MultinomialNB``.\n",
    "Append results in the score data frame to ease comparisons between methods (using the dataframe method ``append``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could have expected multinomial Naive Bayes would perform better than Bernoulli's NB for text classification.\n",
    "However here SMS are short & poor texts and classification is already excellent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Same question but using a calibrated version of the multinomial Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibrated multinomial NB seems to output probabilities of better quality (i.e lower average cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "So far, we have tested off-the-shelf Scikit-learn implementations of Naive Bayes for spam detection (i.e. Bernouilli and multinomial naive Bayes).\n",
    "\n",
    "How could we improve performance?\n",
    "\n",
    "Of course we could choose a better, more complex method like random forests that do not make the strong hypothesis of conditional independence between features when conditionned on the class.\n",
    "Or if we want to avoid the oversimplifying model of bag of words, we could apply a sophisticated Deep Learning model used in *Natural Language Processing* (NLP). But here, we just want to stick to Naive Bayes.\n",
    "\n",
    "Another way is to inject some prior knowledge we know about the problem. This knowledge can take the form of new handcrafted features added to the data. This is called *feature engineering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating named entities through categories\n",
    "\n",
    "Remember we have ignored words that were not purely alphabetical. Looking at the list of removed words, many are what are called *named entities*, like phone numbers, postal codes, etc\n",
    "Removing phone numbers is a pity since spams are more likely to contain phone numbers, especially pay phone numbers. Of course including phone numbers as features do not make sense as they are many and do not generalize. However we could add an single additional feature telling whether a message contains (at least) one phone number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing is to check whether our hypothesis is true before integrating phone numbers in our classifier.\n",
    "\n",
    "**Question:**\n",
    "Run the code below that builds up a boolean pandas serie that indicates which messages contain at least one phone number. How many are they? Is this feature discriminative as we think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize again the messages\n",
    "\n",
    "df2, _ = vectorize_dataset('SMS_spams.txt')\n",
    "print(f\"{len(df2.columns):5} words\")\n",
    "\n",
    "# Extract the sequences of words starting with digits, with at least 7 characters\n",
    "keywords = df2.columns[1:]\n",
    "expr = re.compile('\\d+')\n",
    "numericalWords = [ w for w in keywords if expr.match(w) and len(w) > 6]\n",
    "\n",
    "# Print the results\n",
    "print(f'{len(numericalWords):5} numerical words\\n')\n",
    "print(numericalWords[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract messages with at least one phone number\n",
    "with_phone_number = df2[numericalWords].sum(axis=1) >= 1\n",
    "rows_with_phone_number = df2[with_phone_number]\n",
    "\n",
    "# Counts spam and ham messages among those containing a phone number.\n",
    "cat_with_phone_number = rows_with_phone_number['#class#'].value_counts(normalize=True)\n",
    "print(f\"{len(rows_with_phone_number):5} messages with phone number including:\")\n",
    "print(f\"  - {int(cat_with_phone_number[0]*100):3}% hams\")\n",
    "print(f\"  - {int(cat_with_phone_number[1]*100):3}% spams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for messages without phone numbers.\n",
    "\n",
    "without_phone_number = df2[numericalWords].sum(axis=1) == 0\n",
    "rows_without_phone_number = df2[without_phone_number]\n",
    "cat_without_phone_number = rows_without_phone_number['#class#'].value_counts(normalize=True)\n",
    "print(f\"{len(rows_without_phone_number):5} messages without phone number including:\")\n",
    "print(f\"  - {int(cat_without_phone_number[0]*100):3}% hams\")\n",
    "print(f\"  - {int(cat_without_phone_number[1]*100):3}% spams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This really exceeds our expectations!!\n",
    "Phone numbers are an incredibly discriminative feature!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Reusing array `with_phone_number`, add an additional feature in your dataframe telling whether a message contains a phone number. Then evaluate Naive Bayes on this new dataset to see whether this new feature improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding length of messages\n",
    "\n",
    "Another possibility is to add a feature containing the length of the messages.\n",
    "Before doing this, let's study how message lengths are discriminant with respect to message classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Compute the word length of messages and display the length distribution for ham and spam messages using the Seaborn ``histplot()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Good news: message length is discriminant**\n",
    "\n",
    "However there is a surprise: while one could have assumed spams are shorter than hams on average, we observe the converse!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the length distributions are quite discriminant. So we should investigate further and see what happens when the length of messages is added as an additional feature.\n",
    "\n",
    "We need to choose a parametric distribution for this length: according to the shape of previous plots, a *Poisson distribution* would be a good choice. However scikit-learn only proposes a *Gaussian Naive Bayes*, through the class ``GaussianNB``, to deal with numerical features. Therefore we have two options:\n",
    "\n",
    "- Either we rely on ``GaussianNB``. It's not very realistic but it's straight-forward to implement.\n",
    "- Or we develop our own classifier adapted to Poisson distributions. This might better fit real data but it requires some development effort.\n",
    "\n",
    "Since 1) the classifier performance is already very high and 2) distributions are still very close to bell-shape of normal distributions, the first option is probably the best.\n",
    "\n",
    "However here there is an even better third option that is the best compromise between reality and ease of implementation:\n",
    "We can assume that the logarithm of the message length follows a normal distribution. We say that the length follows a *log-normal distribution*. See [wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution) for details.\n",
    "\n",
    "**Question:**  \n",
    "Visualize the distributions of the message length logarithms for both classes. Are they closer to normal distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently let's see what performance we can get with Naive Bayes when applied to the single feature of the message length logarithm.\n",
    "\n",
    "**Question:**\n",
    "Implement such a classifier using class ``GaussianNB`` and evaluate its performance. Conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing its own classifier\n",
    "\n",
    "Performance is globally excellent. It will be hard to do better.\n",
    "However we can try by integrating the message length as a new input feature.\n",
    "The problem is that *scikit-learn* does not propose implementations of Naive Bayes that can cope with a mix of continuous and categorical distributions. How can we do if we still want to use the scikit-learn environment (cross-validation, etc)?\n",
    "\n",
    "One solution would be to write its own brand new classifier so that it is compatible with the [scikit-learn classifier interface](https://scikit-learn.org/stable/developers/develop.html).\n",
    "Instead we can write our own scikit-learn compatible classifier that will embed:\n",
    "- A classifier``MultinomialNb`` to deal with boolean/discrete features\n",
    "- A classifier``GaussianNb`` to deal with continuous features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, our hybrid classifier will expect columns of the input feature matrix $X$ are partionned in two subsets:\n",
    "- Features whose column index is lower than a threshold (passed as a parameter to the classifier) are Bernoulli variables\n",
    "- Features whose column index is larger than or equal to the threshold are Gaussian variables. \n",
    "\n",
    "\n",
    "**Question:**  \n",
    "Complete code of new class ``HybridNB`` that is compatible with the scikit-learn classifier interface.\n",
    "\n",
    "In addition to the class constructor ``__init__()``, you will have to implement methods ``fit``, ``predict_proba`` and ``predict`` methods in your class ``HybridNB``. Get more information by checking the class template provided in the classifier section of this [user guide](https://sklearn-template.readthedocs.io/en/latest/user_guide.html).\n",
    "You can use the skeleton of code provided below.\n",
    "\n",
    "**Warning: class prior should not be multipled twice. Pass the option ``fit_prior=False`` to one of the underlying classfier (check official documentation).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "class HybridNB(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "        Mixed multinomial & gaussian NB classifier\n",
    "        The nCatFeatures first features are expected to have multinomial distributions\n",
    "        The remaining ones are expected to be normally distributed.\n",
    "    '''\n",
    "    def __init__(self, param1):\n",
    "        self.param1 = param1 # nCatFeatures\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # To complete\n",
    "        pass\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        # To complete\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # To complete\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Add the message length logs at the end of your input feature array $X$ so that discrete and continuous features are correctly arranged. Then test your hybrid classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,nCatFeatures = X.shape\n",
    "Llog = Llog.reshape((n,))              # Log of the message lengths\n",
    "Xmix = np.empty((n,nCatFeatures+1))    # New array\n",
    "Xmix[:,:-1] = X    # First columns are discrete features\n",
    "Xmix[:,-1] = Llog  # Last column is the message length logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = CalibratedClassifierCV(HybridNB(nCatFeatures), cv=10, method='isotonic')\n",
    "scores.test('Hybrid NB', clf, Xmix, Y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "You have experienced many things in this labwork: Scikit-learn, Pandas and Seaborn librairies, different versions of Naive Bayes and above all, general notions such as cross validation, probability calibration, cost-sensitive classification, AUC & ROC curve, feature engineering, data cleansing, etc.\n",
    "\n",
    "But even more importantly, you have seen that building an efficient classifier is an iterative process requiring rigour, insight and criticism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
